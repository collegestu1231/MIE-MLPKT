import logging as log
import torch
from torch.utils import data
import pandas as pd

import copy

class KTDataset(data.Dataset):
    def __init__(self, args,problem_number, batch_size, seq_len, mode):

        self.problem_number = problem_number

        self.batch_size = batch_size
        self.seq_len = seq_len
        self.path = args.data_dir
        self.model = args.model
        self.device = args.device




        self.mode = mode
        log.info('Processing data...')
        self.process(self.path, self.mode)
        if self.mode == 'Train':
            self.data_size = len(self.data_dict['train_problem_tensor'])
        elif self.mode == 'Valid':
            self.data_size = len(self.data_dict['valid_problem_tensor'])
        elif self.mode == 'Test':
            self.data_size = len(self.data_dict['test_problem_tensor'])

        log.info('Processing data done!')

    def __len__(self):
        return self.data_size

    def __getitem__(self, i):
        if self.mode == 'Train':
            # print(len(self.data_dict['train_problem_tensor'][i]))
            # print(len(self.data_dict['train_code_list'][i]))
            return self.data_dict['train_problem_tensor'][i], self.data_dict['train_skills_tensor'][i],\
                self.data_dict['train_label_tensor'][i],self.data_dict['train_note_tensor'][i],\
                self.data_dict['train_reasons_tensor'][i],self.data_dict['train_time_stamp_tensor'][i]
        elif self.mode == 'Valid':

            return self.data_dict['valid_problem_tensor'][i], self.data_dict['valid_skills_tensor'][i],\
                self.data_dict['valid_label_tensor'][i],self.data_dict['valid_note_tensor'][i],\
                self.data_dict['valid_reasons_tensor'][i],self.data_dict['valid_time_stamp_tensor'][i]
        elif self.mode == 'Test':

            return self.data_dict['test_problem_tensor'][i], self.data_dict['test_skills_tensor'][i],\
                self.data_dict['test_label_tensor'][i],self.data_dict['test_note_tensor'][i],\
                self.data_dict['test_reasons_tensor'][i],self.data_dict['test_time_stamp_tensor'][i]

    def pad_and_convert(self, lists, target_length, padding_value=0):
        """
        Pad each sublist in the given list of lists to the target length and convert to a Torch IntTensor.

        Args:
        lists (list of list of int): The list of lists to pad.
        target_length (int): The desired length of each sublist.
        padding_value (int, optional): The value to use for padding. Defaults to 0.

        Returns:
        torch.IntTensor: A tensor of shape (len(lists), target_length) where each sublist is padded.
        """
        padded_lists = []
        for sublist in lists:
            # Truncate the list if it's longer than the target length or pad it if it's shorter
            padded_list = sublist[:target_length] + [padding_value] * (target_length - len(sublist))
            padded_lists.append(padded_list)

        # Convert the list of lists into a tensor of type int
        # print(padded_lists[0][0])
        return torch.tensor(padded_lists, dtype=torch.int32)
    # def pad_and_convert(self, lists, target_length, padding_value=0):
    #     """
    #     Pad each sublist in the given list of lists to the target length and convert to a Torch IntTensor.
    #
    #     Args:
    #     lists (list of list of int): The list of lists to pad.
    #     target_length (int): The desired length of each sublist.
    #     padding_value (int, optional): The value to use for padding. Defaults to 0.
    #
    #     Returns:
    #     torch.IntTensor: A tensor of shape (len(lists), target_length) where each sublist is padded.
    #     """
    #     print("\n" + "=" * 80)
    #     print("ğŸ” pad_and_convert è°ƒè¯•ä¿¡æ¯")
    #     print("=" * 80)
    #     print(f"æ€»åˆ—è¡¨æ•°é‡: {len(lists)}")
    #     print(f"ç›®æ ‡é•¿åº¦: {target_length}")
    #     print(f"å¡«å……å€¼: {padding_value}")
    #
    #     padded_lists = []
    #
    #     for idx, sublist in enumerate(lists):
    #         print(f"\n--- å¤„ç†ç¬¬ {idx} ä¸ªåˆ—è¡¨ ---")
    #         print(f"åŸå§‹ sublist ç±»å‹: {type(sublist)}")
    #         print(f"åŸå§‹ sublist é•¿åº¦: {len(sublist) if isinstance(sublist, list) else 'N/A'}")
    #         print(f"åŸå§‹ sublist å†…å®¹: {sublist}")
    #
    #         # æ£€æŸ¥ sublist ä¸­çš„æ¯ä¸ªå…ƒç´ 
    #         if isinstance(sublist, list):
    #             for i, element in enumerate(sublist):
    #                 if not isinstance(element, (int, float)):
    #                     print(f"âš ï¸  è­¦å‘Š: ç¬¬ {i} ä¸ªå…ƒç´ ä¸æ˜¯æ•°å­—!")
    #                     print(f"   å…ƒç´ å€¼: {element}")
    #                     print(f"   å…ƒç´ ç±»å‹: {type(element)}")
    #                     print(f"   è¿™ä¼šå¯¼è‡´ torch.tensor æŠ¥é”™!")
    #
    #         # Truncate the list if it's longer than the target length or pad it if it's shorter
    #         try:
    #             padded_list = sublist[:target_length] + [padding_value] * (target_length - len(sublist))
    #             print(f"å¡«å……åé•¿åº¦: {len(padded_list)}")
    #             print(f"å¡«å……åå†…å®¹: {padded_list}")
    #
    #             # å†æ¬¡æ£€æŸ¥å¡«å……åçš„åˆ—è¡¨
    #             for i, element in enumerate(padded_list):
    #                 if not isinstance(element, (int, float)):
    #                     print(f"âŒ é”™è¯¯: å¡«å……åç¬¬ {i} ä¸ªå…ƒç´ ä»ç„¶ä¸æ˜¯æ•°å­—!")
    #                     print(f"   å…ƒç´ å€¼: {element}")
    #                     print(f"   å…ƒç´ ç±»å‹: {type(element)}")
    #
    #             padded_lists.append(padded_list)
    #         except Exception as e:
    #             print(f"âŒ å¡«å……è¿‡ç¨‹å‡ºé”™: {e}")
    #             raise
    #
    #     print("\n" + "=" * 80)
    #     print(f"å³å°†è½¬æ¢ä¸º tensor çš„æ•°æ®:")
    #     print(f"æ€»å…± {len(padded_lists)} ä¸ªåˆ—è¡¨")
    #
    #     # æ£€æŸ¥ç¬¬ä¸€ä¸ªåˆ—è¡¨çš„è¯¦ç»†ä¿¡æ¯
    #     if padded_lists:
    #         print(f"\nç¬¬ä¸€ä¸ªåˆ—è¡¨è¯¦æƒ…:")
    #         print(f"  é•¿åº¦: {len(padded_lists[0])}")
    #         print(f"  å†…å®¹: {padded_lists[0]}")
    #         print(f"  ç¬¬ä¸€ä¸ªå…ƒç´ : {padded_lists[0][0]}, ç±»å‹: {type(padded_lists[0][0])}")
    #
    #     # Convert the list of lists into a tensor of type int
    #     try:
    #         print("\nå°è¯•è½¬æ¢ä¸º torch.tensor...")
    #         result = torch.tensor(padded_lists, dtype=torch.int32)
    #         print(f"âœ… è½¬æ¢æˆåŠŸ! Tensor shape: {result.shape}")
    #         return result
    #     except TypeError as e:
    #         print(f"\nâŒâŒâŒ torch.tensor è½¬æ¢å¤±è´¥! âŒâŒâŒ")
    #         print(f"é”™è¯¯ä¿¡æ¯: {e}")
    #         print(f"\nè¯¦ç»†åˆ†æ:")
    #
    #         # æ‰¾å‡ºæœ‰é—®é¢˜çš„å…ƒç´ 
    #         for i, lst in enumerate(padded_lists):
    #             for j, element in enumerate(lst):
    #                 if isinstance(element, str):
    #                     print(f"  é—®é¢˜ä½ç½®: ç¬¬ {i} ä¸ªåˆ—è¡¨çš„ç¬¬ {j} ä¸ªå…ƒç´ ")
    #                     print(f"  é—®é¢˜å…ƒç´ : '{element}' (ç±»å‹: {type(element).__name__})")
    #                 elif not isinstance(element, (int, float)):
    #                     print(f"  é—®é¢˜ä½ç½®: ç¬¬ {i} ä¸ªåˆ—è¡¨çš„ç¬¬ {j} ä¸ªå…ƒç´ ")
    #                     print(f"  é—®é¢˜å…ƒç´ : {element} (ç±»å‹: {type(element).__name__})")
    #
    #         raise

    def gen_list(self, subject_ids, requirements, max_length, column_type=None):

        if column_type == 'Label':
            subject_labels = {}
            for subject_id, requirement in zip(subject_ids, requirements):
                if subject_id not in subject_labels:
                    subject_labels[subject_id] = []

                subject_labels[subject_id].append(requirement)

            result = []
            note = []
            for subject_id in subject_labels.keys():
                requirements = copy.deepcopy(subject_labels[subject_id])  # ä½¿ç”¨æ·±æ‹·è´
                temp_note = copy.deepcopy(subject_labels[subject_id])  # ä½¿ç”¨æ·±æ‹·è´

                if len(requirements) < max_length:
                    padding_length = max_length - len(requirements)

                    requirements.extend([0] * padding_length)
                    temp_note.extend([-1] * padding_length)
                    # print([-1] * (max_length - len(requirements)))

                result.append(requirements)

                note.append(temp_note)

            return result, note

        elif column_type in ['Problem','ErrorNum']:
            subject_problems = {}

            for subject_id, problems in zip(subject_ids, requirements):
                if subject_id not in subject_problems:
                    subject_problems[subject_id] = []

                subject_problems[subject_id].append(problems)

            result = []
            note = []
            for subject_id in subject_problems.keys():

                requirements = subject_problems[subject_id]

                if len(requirements) < max_length:
                    requirements.extend([0] * (max_length - len(requirements)))
                result.append(requirements)
            return result

        elif column_type == "TimeInterval":
            subject_TimeInterval = {}

            for subject_id, ServerTimestamp in zip(subject_ids, requirements):
                if subject_id not in subject_TimeInterval:
                    subject_TimeInterval[subject_id] = []

                subject_TimeInterval[subject_id].append(ServerTimestamp)

            result = []
            for subject_id in subject_TimeInterval.keys():

                requirements = subject_TimeInterval[subject_id]

                if len(requirements) < max_length:
                    requirements.extend([0] * (max_length - len(requirements)))


                result.append(requirements)

            return result

        elif column_type == 'Code':
            subject_requirements = {}
            # éå† SubjectID å’Œ Requirement çš„ç»„åˆï¼Œä¿æŒåŸå§‹é¡ºåº
            for subject_id, requirement in zip(subject_ids, requirements):
                # å¦‚æœ SubjectID å·²ç»åœ¨å­—å…¸ä¸­ï¼Œæ·»åŠ  Requirementï¼Œå¦åˆ™åˆ›å»ºæ–°åˆ—è¡¨
                if subject_id not in subject_requirements:
                    subject_requirements[subject_id] = []

                subject_requirements[subject_id].append(requirement)

            # ç»´æŠ¤åŸå§‹é¡ºåºï¼Œä½¿ç”¨ä¸€ä¸ªåˆ—è¡¨æ¥å­˜å‚¨æ¯ä¸ªå­¦ç”Ÿçš„ Requirement åˆ—è¡¨
            result = []
            for subject_id in subject_requirements.keys():
                # è·å–è¯¥ SubjectID å¯¹åº”çš„ Requirement åˆ—è¡¨
                requirements = subject_requirements[subject_id]

                # å¦‚æœ Requirement æ•°é‡å°‘äº max_lengthï¼Œåˆ™ç”¨ "ç©ºè¯­å¥" å¡«å……

                if len(requirements) < max_length:
                    requirements.extend(['public int nullFunction() {}'] * (max_length - len(requirements)))
                    # print(requirements)

                # å°†å¡«å……åçš„åˆ—è¡¨æ·»åŠ åˆ°ç»“æœä¸­
                result.append(requirements)
            return result
        elif column_type == 'Skills':
            subject_requirements = {}
            for subject_id, requirement in zip(subject_ids, requirements):
                if subject_id not in subject_requirements:
                    subject_requirements[subject_id] = []
                subject_requirements[subject_id].append(requirement)

            # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ä¸‰ç»´å¼ é‡
            result = torch.empty(0, 0, max_length, dtype=torch.int32)

            # å¤„ç†æ¯ä¸ª SubjectID çš„éœ€æ±‚å¹¶æŒ‰æœŸæœ›çš„å½¢çŠ¶æ‹¼æ¥
            for subject_id in subject_requirements.keys():
                requirements = subject_requirements[subject_id]
                requirements_tensor = self.pad_and_convert(requirements, max_length)

                if requirements_tensor.shape[0] < self.seq_len:
                    padding_size = self.seq_len - requirements_tensor.shape[0]
                    # åœ¨ç¬¬äºŒç»´åº¦ï¼ˆåˆ—ï¼‰å¡«å……0
                    pad_tensor = torch.zeros([padding_size, max_length])

                    requirements_tensor = torch.cat([requirements_tensor, pad_tensor], dim=0)

                # ä¸ºäº†æ»¡è¶³å½¢çŠ¶[num_SubjectID, Seq_len, 6]ï¼Œéœ€è¦å¢åŠ ä¸€ç»´
                requirements_tensor = requirements_tensor.unsqueeze(0)  # è¿™é‡Œå°†å½¢çŠ¶ä»[Seq_len, 6]å˜ä¸º[1, Seq_len, 6]
                if result.shape[1:] == (0, max_length):  # å¦‚æœresultè¿˜æ˜¯åˆå§‹ç©ºçŠ¶æ€ï¼Œè®¾ç½®æ­£ç¡®çš„å½¢çŠ¶
                    result = requirements_tensor
                else:
                    result = torch.cat([result, requirements_tensor], dim=0)  # æ²¿ç€ç¬¬ä¸€ä¸ªç»´åº¦ï¼ˆSubjectIDç»´åº¦ï¼‰æ‹¼æ¥
            return result

    def process(self, path, mode='Train'):

        if mode == 'Train':
            print("Training data reading...")

            train_table = pd.read_csv('../../data/' + path + '/'+self.model+'/q_a_train_detailed.csv')
            train_problemid = list(train_table['ProblemID'])
            train_subjectid = list(train_table['SubjectID'])
            train_answer = list(train_table['Label'])
            train_time_stamp = list(train_table['TimeInterval'])
            train_reasons = list(train_table['Reason'].apply(eval))
            train_skills = list(train_table['Skills'].apply(eval))  # ä¸€ä¸ªé—®é¢˜æœ€å¤šåŒ…å«6ä¸ªskill

            train_label_list, train_note_list = self.gen_list(train_subjectid, train_answer, max_length=self.seq_len,
                                                              column_type="Label")  # [num_stu,seq_len]ï¼Œå‰è€…ç”¨0å¡«å……ï¼Œåè€…ç”¨1å¡«å……
            train_problem_list = self.gen_list(train_subjectid, train_problemid, max_length=self.seq_len,
                                               column_type='Problem')  # [num_stu,seq_len]
            train_time_stamp_list = self.gen_list(train_subjectid,train_time_stamp,max_length=self.seq_len,column_type='TimeInterval')


            train_label_tensor = torch.FloatTensor(train_label_list)  # torch.Size([num_stu,seq_len])
            train_note_tensor = torch.FloatTensor(train_note_list)
            train_problem_tensor = torch.IntTensor(train_problem_list)  # torch.Size([num_stu,seq_len])


            train_skills_tensor = self.gen_list(train_subjectid, train_skills, max_length=8,
                                                column_type='Skills')  # torch.Size([num_stu,seq_len,k])
            train_reasons_tensor = self.gen_list(train_subjectid,train_reasons,max_length=5,
                                                 column_type='Skills')
            train_time_stamp_tensor = torch.FloatTensor(train_time_stamp_list)
            # print('ä»€ä¹ˆç½å¤´æˆ‘è¯´',train_time_stamp_tensor.shape)
            self.data_dict = {
                'train_problem_tensor': train_problem_tensor,  # [num_stu,seq_len]
                'train_skills_tensor': train_skills_tensor,
                'train_label_tensor': train_label_tensor,  # [num_stu,seq_len]
                'train_note_tensor': train_note_tensor,
                'train_reasons_tensor': train_reasons_tensor,
                'train_time_stamp_tensor':train_time_stamp_tensor

            }
            return self.data_dict

        elif mode == 'Valid':
            print("Validation data reading...")

            valid_table = pd.read_csv('../../data/' + path + '/' + self.model + '/q_a_valid_detailed.csv')
            valid_problemid = list(valid_table['ProblemID'])
            valid_subjectid = list(valid_table['SubjectID'])
            valid_answer = list(valid_table['Label'])
            valid_time_stamp = list(valid_table['TimeInterval'])
            valid_reasons = list(valid_table['Reason'].apply(eval))
            valid_skills = list(valid_table['Skills'].apply(eval))  # ä¸€ä¸ªé—®é¢˜æœ€å¤šåŒ…å«6ä¸ªskill

            valid_label_list, valid_note_list = self.gen_list(valid_subjectid, valid_answer, max_length=self.seq_len,
                                                              column_type="Label")  # [num_stu,seq_len]ï¼Œå‰è€…ç”¨0å¡«å……ï¼Œåè€…ç”¨1å¡«å……
            valid_problem_list = self.gen_list(valid_subjectid, valid_problemid, max_length=self.seq_len,
                                               column_type='Problem')  # [num_stu,seq_len]
            valid_time_stamp_list = self.gen_list(valid_subjectid, valid_time_stamp, max_length=self.seq_len,
                                                  column_type='TimeInterval')

            valid_label_tensor = torch.FloatTensor(valid_label_list)  # torch.Size([num_stu,seq_len])
            valid_note_tensor = torch.FloatTensor(valid_note_list)
            valid_problem_tensor = torch.IntTensor(valid_problem_list)  # torch.Size([num_stu,seq_len])

            valid_skills_tensor = self.gen_list(valid_subjectid, valid_skills, max_length=8,
                                                column_type='Skills')  # torch.Size([num_stu,seq_len,k])
            valid_reasons_tensor = self.gen_list(valid_subjectid, valid_reasons, max_length=5,
                                                 column_type='Skills')
            valid_time_stamp_tensor = torch.FloatTensor(valid_time_stamp_list)
            # print('ä»€ä¹ˆç½å¤´æˆ‘è¯´',valid_time_stamp_tensor.shape)
            self.data_dict = {
                'valid_problem_tensor': valid_problem_tensor,  # [num_stu,seq_len]
                'valid_skills_tensor': valid_skills_tensor,
                'valid_label_tensor': valid_label_tensor,  # [num_stu,seq_len]
                'valid_note_tensor': valid_note_tensor,
                'valid_reasons_tensor': valid_reasons_tensor,
                'valid_time_stamp_tensor': valid_time_stamp_tensor
            }
            return self.data_dict
        elif mode == "Test":
            print("Test data reading...")

            test_table = pd.read_csv('../../data/' + path + '/' + self.model + '/q_a_test_detailed.csv')
            test_problemid = list(test_table['ProblemID'])
            test_subjectid = list(test_table['SubjectID'])
            test_answer = list(test_table['Label'])
            test_time_stamp = list(test_table['TimeInterval'])
            test_reasons = list(test_table['Reason'].apply(eval))
            test_skills = list(test_table['Skills'].apply(eval))  # ä¸€ä¸ªé—®é¢˜æœ€å¤šåŒ…å«6ä¸ªskill

            test_label_list, test_note_list = self.gen_list(test_subjectid, test_answer, max_length=self.seq_len,
                                                            column_type="Label")  # [num_stu,seq_len]ï¼Œå‰è€…ç”¨0å¡«å……ï¼Œåè€…ç”¨1å¡«å……
            test_problem_list = self.gen_list(test_subjectid, test_problemid, max_length=self.seq_len,
                                              column_type='Problem')  # [num_stu,seq_len]
            test_time_stamp_list = self.gen_list(test_subjectid, test_time_stamp, max_length=self.seq_len,
                                                 column_type='TimeInterval')

            test_label_tensor = torch.FloatTensor(test_label_list)  # torch.Size([num_stu,seq_len])
            test_note_tensor = torch.FloatTensor(test_note_list)
            test_problem_tensor = torch.IntTensor(test_problem_list)  # torch.Size([num_stu,seq_len])

            test_skills_tensor = self.gen_list(test_subjectid, test_skills, max_length=8,
                                               column_type='Skills')  # torch.Size([num_stu,seq_len,k])
            test_reasons_tensor = self.gen_list(test_subjectid, test_reasons, max_length=5,
                                                column_type='Skills')
            test_time_stamp_tensor = torch.FloatTensor(test_time_stamp_list)
            # print('ä»€ä¹ˆç½å¤´æˆ‘è¯´',test_time_stamp_tensor.shape)
            self.data_dict = {
                'test_problem_tensor': test_problem_tensor,  # [num_stu,seq_len]
                'test_skills_tensor': test_skills_tensor,
                'test_label_tensor': test_label_tensor,  # [num_stu,seq_len]
                'test_note_tensor': test_note_tensor,
                'test_reasons_tensor': test_reasons_tensor,
                'test_time_stamp_tensor': test_time_stamp_tensor
            }
            return self.data_dict









