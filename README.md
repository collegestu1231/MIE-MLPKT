# MIE-MLPKT
This repository is built for the paper MIE+MLPKT: Leveraging LLM and Multiscale Knowledge States to Improve Knowledge Tracing in Programming Tasks.
> **Author: mingxing Shao**, **tiancheng Zhang**
## MIE Framework
As illustrated in Figure 2, we design an LLM-Driven Interaction Enrichment Framework to provide reasonable score labels and code analysis results for the three-level knowledge tracing framework. 

<div align="center">
  <img src="MIE.png" alt="MIE">
</div>

### Rational Score Evaluation
To adapt the labels in programming datasets for knowledge tracing models, for the code $m_t^i \in \mathcal{M}$ submitted by student $S_i$ at time step $t$, with corresponding ID $\hat{m}_t^i$ and problem description $PD_t^i$, we design a scoring prompt denoted as $P^{\text{s}}$ to obtain the code score at this time step. As shown in Figure 2(a), it mainly includes the inherent data in the dataset ($\rm Data$), namely the student's code $m_t^i$ along with its corresponding ID $\hat{m}_t^i$ and the problem stem $PD_t^i$, scoring criteria with 5 levels ($\rm Level$), factors to consider during the scoring process ($\rm Factors$), and other notes ($\rm Notes$), such as the return format, etc. The entire process can be represented as:

$$ \hat l^i_t = \text{LLM}(P^{\text{s}}(\text{Data}(m^i_t \mid PD^i_t \mid \hat{m}^i_t) \mid \text{Level} \mid \text{Factors} \mid \text{Notes})), \forall S_i \in \mathcal S $$

where $\mid$ represents combination and $\hat l^i_t \in \hat{\mathcal L}$. At this point, we have obtained the set of scores generated by LLM $\hat{\mathcal L}$.

### Domain Causal Analysis
As shown in Figure 2(b), to mitigate the impact of noise arising from high uncertainty in programming tasks on the assessment of students' knowledge states, for student $S_i$ who submits code $m_t^i \in \mathcal{M}$ at time step $t$, along with its corresponding ID $\hat{m}_t^i$, problem description $PD_t^i$, and the code score $\hat{l}_t^i$ obtained from the previous step, we design a reasoning prompt $P^r$ to guide the LLM in inferring the correctness at the logical level and the correctness or error causes at the syntactic level. The resulting analysis is encoded using a 5-dimensional binary vector $R_t^i$ as follows:

- First dimension: Logical correctness.
- Second dimension: Syntactic correctness.
- Third dimension: Syntactic errors due to carelessness.
- Fourth dimension: Syntactic errors due to insufficient proficiency.
- Fifth dimension: Logical errors.

For example, if the LLM determines that the submitted code $m_t^i \in \mathcal{M}$ is logically correct but contains syntactic errors due to carelessness (e.g., unclosed parentheses), the returned vector is $R_t^i = [1, 0, 1, 0, 0]$. In another example, if errors occur in both logical and syntactic aspects, the returned vector is $R_t^i = [0, 0, 0, 1, 1]$. The aforementioned process can be formally expressed as:

$$ R^i_t = \text{LLM}(P^r(\text{Data}(m_t^i \mid \hat{m}_t^i \mid PD_t^i \mid \hat{l}_t^i) \mid \text{Rules} \mid \text{Desc} \mid \text{Notes})), \forall S_i \in \mathcal S $$

where $R^i_t \in \mathcal R$. Thus, we obtain the collection of code analyses $\mathcal R$ as mentioned earlier.

## MLPKT Framework
<div align="center">
  <img src="MLPKT.png" alt="MLPKT">
</div>

## Overview
This paper highlights three unique characteristics of programming knowledge tracing tasks compared to traditional subject-based knowledge tracing, which often lead to suboptimal performance of existing KT models on programming benchmarks. To address these challenges, we propose the MIE+MLPKT framework. The MIE component uses carefully designed prompts to extract reasonable scoring labels and multi-level error cause analyses from large language models (LLMs), mitigating label continuity and irrationality issues while reducing high uncertainty. To handle the multi-layered nature of student proficiency in programming tasks, we introduce MLPKT, a three-tiered knowledge tracing framework that integrates error analysis vectors from MIE. Extensive experiments across three datasets and 18 baselines validate the effectiveness of our framework. In future work, we aim to evaluate MIE+MLPKT in real-world educational settings.
## Reproduce
If you want to run the code, you need to first download the "data" profile, available at: https://drive.google.com/drive/folders/1jdoNIi4GNeYeps6R3tCDsz3GAaEP7KK6?usp=drive_link. It is worth noting that, as the paper has not yet been accepted, apart from the labeled dataset of the deepseek-V3.1 version for Code-S, we have currently only open-sourced the first 5000 ratings and reasoning extraction data for each dataset. Once the paper is successfully accepted, we will immediately open-source the complete data.
